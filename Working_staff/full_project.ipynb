{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "from utils import load_json_file\n",
    "from model_components.bridgetower_embeddings import (\n",
    "    BridgeTowerEmbeddings\n",
    ")\n",
    "\n",
    "from model_components.multimodal_lancedb import MultimodalLanceDB\n",
    "# from model_components.client import PredictionGuardClient\n",
    "from model_components.lvlm import LVLM\n",
    "from PIL import Image\n",
    "from langchain_core.runnables import (\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    "    RunnableLambda\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for prepopulated data\n",
    "# TBL_NAME = \"demo_tbl\"\n",
    "\n",
    "# initializeing the vector store that we have previouslly constructed\n",
    "\n",
    "# declare host file\n",
    "LANCEDB_HOST_FILE = \"./shared_data/.lancedb\"\n",
    "\n",
    "# declare table name \n",
    "TBL_NAME = \"test_tbl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets initialize the bridge tower emmbedding model\n",
    "embedder = BridgeTowerEmbeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets create the retriveval\n",
    "# Creating a LanceDB vector store\n",
    "vectorstore = MultimodalLanceDB(\n",
    "    uri=LANCEDB_HOST_FILE,\n",
    "    embedding = embedder,\n",
    "    table_name = TBL_NAME\n",
    ")\n",
    "\n",
    "# creating a retriver for the vector store\n",
    "# with search type=\"similarity\" and search_kwargs={\"k\":1} \n",
    "retriever_module = vectorstore.as_retriever(\n",
    "    search_type='similarity',\n",
    "    search_kwargs={\"k\":1}\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags=['MultimodalLanceDB', 'BridgeTowerEmbeddings'] vectorstore=<model_components.multimodal_lancedb.MultimodalLanceDB object at 0x000001C9A58F21F0> search_kwargs={'k': 1}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No api_key provided or in environment. Please provide the api_key as client = PredictionGuard(api_key=<your_api_key>) or as PREDICTIONGUARD_API_KEY in your environment.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat do the astronauts feel about their work?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(retriever_module)\n\u001b[1;32m----> 6\u001b[0m retrieved_video_segments \u001b[38;5;241m=\u001b[39m \u001b[43mretriever_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# get the first retrieved video segment\u001b[39;00m\n\u001b[0;32m      9\u001b[0m retrieved_video_segment \u001b[38;5;241m=\u001b[39m retrieved_video_segments[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\langchain_core\\retrievers.py:254\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    253\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_error(e)\n\u001b[1;32m--> 254\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    256\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_end(\n\u001b[0;32m    257\u001b[0m         result,\n\u001b[0;32m    258\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\langchain_core\\retrievers.py:247\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    245\u001b[0m _kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[1;32m--> 247\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\n\u001b[0;32m    248\u001b[0m         \u001b[38;5;28minput\u001b[39m, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs\n\u001b[0;32m    249\u001b[0m     )\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    251\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\langchain_core\\vectorstores\\base.py:1080\u001b[0m, in \u001b[0;36mVectorStoreRetriever._get_relevant_documents\u001b[1;34m(self, query, run_manager)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_relevant_documents\u001b[39m(\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m, run_manager: CallbackManagerForRetrieverRun\n\u001b[0;32m   1078\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1080\u001b[0m         docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39msimilarity_search(query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_kwargs)\n\u001b[0;32m   1081\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity_score_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1082\u001b[0m         docs_and_similarities \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1083\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39msimilarity_search_with_relevance_scores(\n\u001b[0;32m   1084\u001b[0m                 query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_kwargs\n\u001b[0;32m   1085\u001b[0m             )\n\u001b[0;32m   1086\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\langchain_community\\vectorstores\\lancedb.py:524\u001b[0m, in \u001b[0;36mLanceDB.similarity_search\u001b[1;34m(self, query, k, name, filter, fts, **kwargs)\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity_search\u001b[39m(\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    502\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    508\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m    509\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return documents most similar to the query\u001b[39;00m\n\u001b[0;32m    510\u001b[0m \n\u001b[0;32m    511\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;124;03m        List of documents most similar to the query.\u001b[39;00m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 524\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity_search_with_score(\n\u001b[0;32m    525\u001b[0m         query\u001b[38;5;241m=\u001b[39mquery, k\u001b[38;5;241m=\u001b[39mk, name\u001b[38;5;241m=\u001b[39mname, \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfilter\u001b[39m, fts\u001b[38;5;241m=\u001b[39mfts, score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    526\u001b[0m     )\n\u001b[0;32m    527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\langchain_community\\vectorstores\\lancedb.py:496\u001b[0m, in \u001b[0;36mLanceDB.similarity_search_with_score\u001b[1;34m(self, query, k, filter, **kwargs)\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    493\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull text/ Hybrid search is not supported in LanceDB Cloud yet.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    494\u001b[0m         )\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 496\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    497\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query(embedding, k, \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfilter\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults_to_docs(res, score\u001b[38;5;241m=\u001b[39mscore)\n",
      "File \u001b[1;32mc:\\Users\\Marianna\\Desktop\\Chat with video\\Working_staff\\model_components\\bridgetower_embeddings.py:36\u001b[0m, in \u001b[0;36mBridgeTowerEmbeddings.embed_query\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m     28\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Embed a query using BridgeTower.\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03m        Embeddings for the text.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Marianna\\Desktop\\Chat with video\\Working_staff\\model_components\\bridgetower_embeddings.py:23\u001b[0m, in \u001b[0;36mBridgeTowerEmbeddings.embed_documents\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m     21\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[1;32m---> 23\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[43mbt_embedding_from_prediction_guard\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mappend(embedding)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[1;32mc:\\Users\\Marianna\\Desktop\\Chat with video\\Working_staff\\utils.py:307\u001b[0m, in \u001b[0;36mbt_embedding_from_prediction_guard\u001b[1;34m(prompt, base64_image)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbt_embedding_from_prediction_guard\u001b[39m(prompt, base64_image):\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;66;03m# get PredictionGuard client\u001b[39;00m\n\u001b[1;32m--> 307\u001b[0m     client \u001b[38;5;241m=\u001b[39m \u001b[43m_getPredictionGuardClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    308\u001b[0m     message \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,}\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m base64_image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m base64_image \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Marianna\\Desktop\\Chat with video\\Working_staff\\utils.py:490\u001b[0m, in \u001b[0;36m_getPredictionGuardClient\u001b[1;34m()\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_getPredictionGuardClient\u001b[39m():\n\u001b[0;32m    489\u001b[0m     PREDICTION_GUARD_API_KEY \u001b[38;5;241m=\u001b[39m get_prediction_guard_api_key()\n\u001b[1;32m--> 490\u001b[0m     client \u001b[38;5;241m=\u001b[39m \u001b[43mPredictionGuard\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPREDICTION_GUARD_API_KEY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPREDICTION_GUARD_URL_ENDPOINT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    494\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m client\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\predictionguard\\client.py:31\u001b[0m, in \u001b[0;36mPredictionGuard.__init__\u001b[1;34m(self, api_key, url)\u001b[0m\n\u001b[0;32m     28\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPREDICTIONGUARD_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m api_key:\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo api_key provided or in environment. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease provide the api_key as \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient = PredictionGuard(api_key=<your_api_key>) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor as PREDICTIONGUARD_API_KEY in your environment.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m     )\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m url:\n",
      "\u001b[1;31mValueError\u001b[0m: No api_key provided or in environment. Please provide the api_key as client = PredictionGuard(api_key=<your_api_key>) or as PREDICTIONGUARD_API_KEY in your environment."
     ]
    }
   ],
   "source": [
    "# Invoke Retrival with USer query\n",
    "\n",
    "# invoke the retriever for a query\n",
    "query = \"what do the astronauts feel about their work?\"\n",
    "print(retriever_module)\n",
    "retrieved_video_segments = retriever_module.invoke(query)\n",
    "\n",
    "# get the first retrieved video segment\n",
    "retrieved_video_segment = retrieved_video_segments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retrieved_video_segment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# get all metadata of the retrieved video segment\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m retrieved_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mretrieved_video_segment\u001b[49m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# get the extracted frame\u001b[39;00m\n\u001b[0;32m      5\u001b[0m frame_path \u001b[38;5;241m=\u001b[39m retrieved_metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextracted_frame_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'retrieved_video_segment' is not defined"
     ]
    }
   ],
   "source": [
    "# get all metadata of the retrieved video segment\n",
    "retrieved_metadata = retrieved_video_segment.metadata['metadata']\n",
    "\n",
    "# get the extracted frame\n",
    "frame_path = retrieved_metadata['extracted_frame_path']\n",
    "\n",
    "# get the corresponding transcript\n",
    "transcript = retrieved_metadata['transcript']\n",
    "\n",
    "# get the path to video when the frame was extracted\n",
    "timestamp = retrieved_metadata['mid_time_ms']\n",
    "\n",
    "# display\n",
    "print(f\"Transcript:\\n{transcript}\\n\")\n",
    "print(f\"Path to extracted frame: {frame_path}\")\n",
    "print(f\"Path to video: {video_path}\")\n",
    "print(f\"Timestamp in ms when the frame was extracted: {timestamp}\")\n",
    "display(Image.open(frame_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If LVLM doesn't strictly require a client to be passed, and you want to handle inference directly without an external API, you can simply omit the client initialization. In this case, modify the LVLM initialization to not expect a client, or pass None if it's optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LVLM Inference Module\n",
    "# Initialize Client and LVL< for Inference \n",
    "\n",
    "# initialize a client as PredictioGuardClient\n",
    "# client = PredictionGuardClient()\n",
    "client = None\n",
    "\n",
    "# initialize LVLM with the given client\n",
    "lvlm_inference_module = LVLM(client= client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transcript' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Invoke LVLM Inference with User Query\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# This new query is the augumentation of the previous query\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# with the transcript retrived above \u001b[39;00m\n\u001b[0;32m      5\u001b[0m augumented_query_template \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe transcript associated with the image is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{transcript}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{previous_query}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m augumented_query \u001b[38;5;241m=\u001b[39m augumented_query_template\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m---> 11\u001b[0m     transcript\u001b[38;5;241m=\u001b[39m\u001b[43mtranscript\u001b[49m,\n\u001b[0;32m     12\u001b[0m     previous_query \u001b[38;5;241m=\u001b[39m query,\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAugmented query is:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maugumented_query\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transcript' is not defined"
     ]
    }
   ],
   "source": [
    "# Invoke LVLM Inference with User Query\n",
    "\n",
    "# This new query is the augumentation of the previous query\n",
    "# with the transcript retrived above \n",
    "augumented_query_template = (\n",
    "    \"The transcript associated with the image is '{transcript}'.\"\n",
    "    \"{previous_query}\"\n",
    ")\n",
    "\n",
    "augumented_query = augumented_query_template.format(\n",
    "    transcript=transcript,\n",
    "    previous_query = query,\n",
    ")\n",
    "\n",
    "print(f\"Augmented query is:\\n {augumented_query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the augmented query and the retrieved path-to-image\n",
    "# as the input to LVLM inference module\n",
    "\n",
    "inpute = {'prompt':augumented_query, 'image':frame_path}\n",
    "response = lvlm_inference_module.invoke(input)\n",
    "\n",
    "# display the response\n",
    "print('LVLM Response:')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt processing module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_processing(input):\n",
    "    # get the retrieved results and user's query\n",
    "    retrieved_results = input['retrieved_results']\n",
    "    user_query = input['user_query']\n",
    "\n",
    "    # get the first retrieved result by default\n",
    "    retrieved_result = retrieved_results[0]\n",
    "    prompt_template = (\n",
    "        \"The transcript associated with the image is '{transcript}'.\"\n",
    "        \"{user_query}\"\n",
    "    )\n",
    "\n",
    "    # get all metadata of the retrieved video segment\n",
    "    retrieved_metadata = retrieved_result.metadata['metadata']\n",
    "\n",
    "    # get coressponding transcript\n",
    "    transcript_metadata = retrieved_metadata['transcript']\n",
    "    # get the extracted frame\n",
    "    frame_path = retrieved_metadata['extracted_frame_path']\n",
    "\n",
    "    return {\n",
    "        'prompt': prompt_template.format(\n",
    "            transcript=transcript,\n",
    "            user_query = user_query \n",
    "\n",
    "        ), \n",
    "        'image': frame_path\n",
    "    }\n",
    "\n",
    "# initialize prompt processing module\n",
    "# as a langchain RunnableLambda of function prompt_processing\n",
    "prompt_processing_module = RunnableLambda(prompt_processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke Prompt Processing Module with query and the retrieved results above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the user query and the retrived results above\n",
    "input_to_lvlm = prompt_processing_module.invoke(\n",
    "    {\n",
    "        'retrieved_results': retrieved_video_segments,\n",
    "        'user_query': query\n",
    "})\n",
    "\n",
    "# display output of prompt processing module\n",
    "# which is the input to LVLM Inference module\n",
    "print(input_to_lvlm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Multimodal RAG System as a Chain in LangChain¶\n",
    "We are going to make use of the followings from Langchain:\n",
    "\n",
    "The RunnableParallel primitive is essentially a dict whose values are runnables (or things that can be coerced to runnables, like functions). It runs all of its values in parallel, and each value is called with the overall input of the RunnableParallel. The final return value is a dict with the results of each value under its appropriate key.\n",
    "The RunnablePassthrough on its own allows you to pass inputs unchanged. This typically is used in conjuction with RunnableParallel to pass data through to a new key in the map.\n",
    "The RunnableLambda converts a python function into a Runnable. Wrapping a function in a RunnableLambda makes the function usable within either a sync or async context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all the modules into a chain\n",
    "# to create multimodal RAG system\n",
    "\n",
    "mm_rag_chain = (\n",
    "    RunnableParallel({\n",
    "        \"retrieved_results\": retriever_module,\n",
    "        \"user_query\": RunnablePassthrough()\n",
    "    })\n",
    "    | prompt_processing_module\n",
    "    | lvlm_inference_module\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke the Multimodal RAG System with a query\n",
    "query1 = \"What do the astronauts feel about their work?\"\n",
    "final_text_response = mm_rag_chain.invoke(query1)\n",
    "\n",
    "# display\n",
    "print(f\"USER Query: {query1}\")\n",
    "print(f\"MM-RAG Response: {final_text_response}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try another query\n",
    "\n",
    "query2 = \"What is the name of the astronauts?\"\n",
    "final_text_response2 = mm_rag_chain.invoke(query2)\n",
    "# display\n",
    "\n",
    "print(f\"USER Query: {query2}\")\n",
    "print(f\"MM-RAG Response: {final_text_response2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multimodal RAG system showing image/frame retrived \n",
    "mm_rag_chain_with_retrieved_image = (\n",
    "    RunnableParallel({\n",
    "        \"retreived_results\": retriever_module,\n",
    "        \"user_query\": RunnablePassthrough()\n",
    "    })\n",
    "    | prompt_processing_module\n",
    "    | RunnableParallel({\n",
    "        'final_text_output': lvlm_inference_module,\n",
    "        'input_to_lvlm': RunnablePassthrough()\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try again with the query2\n",
    "response3 = mm_rag_chain_with_retrieved_image.invoke(query2)\n",
    "# display\n",
    "print(\"Type of output of mm_Rag_chain_with_retrieved _image is :\")\n",
    "print(type(response3))\n",
    "print(f\"Keys of the dict are {response3.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now extract final text response and path to extracted frame\n",
    "final_text_response3 = response3['final_text_output']\n",
    "path_to_extracted_frame = response3['input_To_lvlm']['image']\n",
    "\n",
    "# display \n",
    "print(f\"USER Query: {query2}\")\n",
    "print(f\"MM-RAG Response: {final_text_response3}\")\n",
    "print(\"Retrieved frame:\")\n",
    "display(Image.open(path_to_extracted_frame))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try again with another query\n",
    "\n",
    "query4 = \"an asronaut's spacewalk\"\n",
    "response4 = mm_rag_chain_with_retrieved_image.invoke(query4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract results\n",
    "final_text_response4 = response4['final_text_output']\n",
    "path_to_extracted_frame4 = response4['final_text_output']\n",
    "path_to_extracted_frame4 = response4['input_to_lvlm']['image']\n",
    "\n",
    "# display\n",
    "print(f\"USER Query: {query4}\")\n",
    "print()\n",
    "print(f\"MM-RAG Response: {final_text_response4}\")\n",
    "print()\n",
    "print(\"Retrieved frame:\")\n",
    "display(Image.open(path_to_extracted_frame4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We would like an astronaut's spacewalk with the earth view behind\n",
    "query5 = (\n",
    "    \"Describe the image of an astronaut's spacewalk\"\n",
    "    \"with an amazing view of the earth from space behind\"\n",
    ")\n",
    "\n",
    "response5 = mm_rag_chain_with_retrieved_image.invoke(query5)\n",
    "\n",
    "# extract results\n",
    "final_text_response5 = response5['final_text_output']\n",
    "path_to_extracted_frame5 = response5['input_to_lvlm']['image']\n",
    "\n",
    "# display\n",
    "print(f\"User Query:  {query5}\")\n",
    "print()\n",
    "print(f\"MM-RAG Response: {final_text_response5}\")\n",
    "print()\n",
    "print(\"Retrieved Frame:\")\n",
    "display(Image.open(path_to_extracted_frame5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slightly change the query5\n",
    "\n",
    "query6 = (\n",
    "    \"An astronaut's spacewalk with \"\n",
    "    \"an amazing view of the earth from space behind\"\n",
    ")\n",
    "response6 = mm_rag_chain_with_retrieved_image.invoke(query6)\n",
    "# extract results\n",
    "final_text_response6 = response6['final_text_output']\n",
    "path_to_extracted_frame6 = response6['input_to_lvlm']['image']\n",
    "# display\n",
    "print(f\"USER Query: {query6}\")\n",
    "print()\n",
    "print(f\"MM-RAG Response: {final_text_response6}\")\n",
    "print()\n",
    "print(\"Retrieved Frame:\")\n",
    "display(Image.open(path_to_extracted_frame6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
